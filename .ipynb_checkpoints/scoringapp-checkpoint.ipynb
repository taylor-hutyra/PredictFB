{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import *\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, Label, Button, Layout, Box\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.4, max_features=None, min_df=0.02,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True... epsilon=0.1, gamma=0.001,\n",
      "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False))])\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('poly_grid.pkl')\n",
    "print(model)\n",
    "scaler = joblib.load('minmax.pkl')\n",
    "print(scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    words = ' '.join([WordNetLemmatizer().lemmatize(y.lower()) for y in tokenized if y.isalpha() and y not in stopwords.words('english')])\n",
    "    prediction = np.array(model.predict([words])).reshape(1,-1)\n",
    "    prop = \"{0:.2f}%\".format(scaler.inverse_transform(prediction).ravel()[0]*100)\n",
    "    return prop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f34eec828642ab952fd529c2b7502d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def clicked(button):\n",
    "    label.value = call_model(text.value)\n",
    "b = widgets.Button(description='Predict',layout=Layout(width='80px', height='40px'))\n",
    "b.on_click(clicked)\n",
    "text = widgets.Textarea(value='Hello World',placeholder='Type something',description='String:',disabled=False)\n",
    "\n",
    "label = widgets.Label(value=\"Hai\")\n",
    "row_layout = Layout(display='flex',flex_flow='row', align_items='stretch')\n",
    "col_layout = Layout(display='flex',flex_flow='col')\n",
    "\n",
    "form = Box([text, Box([b, label], layout=Layout(display='flex', flex_flow='column', width='50%'))], layout=row_layout)\n",
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
